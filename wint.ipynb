{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Train a GPT-2 Text-Generating Model w/ GPU",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/corbittcoder/wint/blob/master/wint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). You can also read my [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information how to use this notebook!\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "6d57d271-7af9-400d-e182-fa395b465cb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "c0bc872e-22de-4fd2-9dbe-2b38c311bb5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 12 14:51:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "1be232d0-dc77-41c9-c599-9bbcf54cc84b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 292Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 111Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 366Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:02, 189Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 333Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 187Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 193Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "c69f4166-5cf8-4c1b-9681-2c7036ef40c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"TrumpFilteredTweets.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "606d9918-8554-4b7e-e938-a1840f7aa81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 726590 tokens\n",
            "Training...\n",
            "[10 | 28.98] loss=3.45 avg=3.45\n",
            "[20 | 50.89] loss=3.44 avg=3.45\n",
            "[30 | 73.29] loss=3.54 avg=3.48\n",
            "[40 | 96.32] loss=3.34 avg=3.44\n",
            "[50 | 120.54] loss=3.31 avg=3.42\n",
            "[60 | 144.72] loss=3.27 avg=3.39\n",
            "[70 | 168.31] loss=3.22 avg=3.37\n",
            "[80 | 192.09] loss=3.28 avg=3.36\n",
            "[90 | 216.14] loss=3.26 avg=3.34\n",
            "[100 | 240.01] loss=3.05 avg=3.31\n",
            "[110 | 263.87] loss=3.05 avg=3.29\n",
            "[120 | 287.76] loss=3.13 avg=3.28\n",
            "[130 | 311.65] loss=3.10 avg=3.26\n",
            "[140 | 335.51] loss=2.90 avg=3.23\n",
            "[150 | 359.40] loss=2.98 avg=3.21\n",
            "[160 | 383.32] loss=2.97 avg=3.20\n",
            "[170 | 407.31] loss=3.06 avg=3.19\n",
            "[180 | 431.39] loss=2.90 avg=3.17\n",
            "[190 | 455.34] loss=2.85 avg=3.15\n",
            "[200 | 479.23] loss=3.23 avg=3.16\n",
            "======== SAMPLE 1 ========\n",
            " Polling (Trump: “TrumpNation” interviewed 801,000 voters) with the most accurate information possible.” A major contributor to the @nytawild@senate  but a total bad guy.\n",
            "@LADJHUTZER @MariaCindavila This interview is a total fabrication! A TOTAL SCAM!\n",
            "What about the FBI being involved in a Russian Coverup? Where was the Collusion or what was it? Why were all of the documents destroyed by Hillary? No matter how much time she uses to destroy the FBI and the @FBI.....\n",
            "....this is just the latest Fake News. If the Democrats and the @FBI are found to be investigating the Collusion then why are people in the Republican Party who have taken over the Democrat Party so closely (with the Democrats)....maybe it should be thrown out of the party for the better (with a Republican) or a.....\n",
            "....Party could handle it. No crime! Now the Republicans are all about bringing in more Fake News!\n",
            "Fake News @FoxNews and @CNN have been at it for years. They are being forced to report Fake News for years...\n",
            "RT @NoreillyDeal @realDonaldTrump: The U.S. has gone from one of the best nations (and the wealthiest) nation in the world to an international leader-with no government leaders. @MariaBartiromo @TuckerCarlson @CNN @MariaBartiromo @CNN @MariaBartiromo @MariaBartiromo @CNN @MariaBartiromo @CNN @MariaBartiromo @CNN @MariaBartiromo @CNN @MariaBartiromo @CNN & more..\n",
            "RT @TuckerCarlson: \"Russia and Trump are using each other as leverage against each other. It's a vicious cycle with an inevitable beginning.\" -- I am a \"tremendous businessman &amp; investor\".\n",
            "RT @MariaBartiromo: I love to see the @POTUS's tweet...\n",
            "RT @MariaBartiromo: The reason Russia & TRUMP are using each other as leverage...\n",
            "RT @MariaBartiromo: The proof to watch: In fact I never did call (or spoke to) the Russian ambassador the first time in the second one but we have...\n",
            "RT @JessicaMueller: “Why did the Dems call the CIA the FBI the FBI the FBI all others…” @CNN\n",
            "RT @MariaBartiromo: It was a perfect call to get what the people’s representatives said. They never said anything wrong. \n",
            "RT @MariaBartiromo: So a lot has happened since I met with the Trump team. They have agreed to the call...\n",
            "RT @marcorubio: #Trump2016: #Trump2016: #Trump2016  #Trump2016  #Trump2016 #Trump2016 #Trump2016 #Trump2016  #Trump2016 #TRUMP2016 #Trump2016 #Trump2016 #Trump2016 #Trump2016 #Phelps #Rapper #Celebrity Apprentice #CelebrityHOF #CelebApprentice\n",
            "RT @marcorubio: #Trump2016: #Trump2016\n",
            "RT @marcorubio:  #Trump2016: #Trump2016 #Trump2016 #Trump2016 #Trump2016 #Trump2016 #Trump2016 #CelebrityI…\n",
            "The reason the Dems are so desperate to get Comey fired is because they don’t understand why they did the same thing the other day to save the FBI &amp; DOJ. They are a total mess with no direction.\n",
            "RT @MariaBartiromo: I am a big fan of President Trump but I’m afraid that the White House may give him one - he’s got a...\n",
            "RT @MariaBartiromo: This is why the Democrats never knew that we’ve gone from an informant-a traitor to our President-A corrupt politician and all-a big liar for treason...\n",
            "RT @MariaBartiromo: I have no doubt that @realDonaldTrump will be using the FBI's  #Fake News and @FBI's  #Fake News...\n",
            "RT @GOPChairwoman: A new poll shows Dems with more power than their rivals in Florida. @MariaBartiromo:…\n",
            "RT @RepDougCollins: @realDonaldTrump I’m pleased to announce that @MariaBartiromo is leading the Republican Party as the #Dem…\n",
            "RT @MariaBartiromo: The #GOPChairwoman @realDonaldTrump has just announced that @MariaBartiromo will lead the GOP...\n",
            "It turns out @MariaBartiromo is running a very strong race against the beautiful Melania!\n",
            "I am very proud to have @DonaldJTrumpJr as our new CEO.\n",
            "All the Republican Congressional candidates @JohnBiden should be ashamed of themselves for running against one of their least trusted candidates.\n",
            "Congratulations to @Maria\n",
            "\n",
            "[210 | 515.34] loss=2.76 avg=3.14\n",
            "[220 | 539.29] loss=2.87 avg=3.12\n",
            "[230 | 563.20] loss=2.93 avg=3.11\n",
            "[240 | 587.09] loss=2.85 avg=3.10\n",
            "[250 | 610.96] loss=2.68 avg=3.08\n",
            "[260 | 634.91] loss=2.92 avg=3.08\n",
            "[270 | 658.88] loss=2.67 avg=3.06\n",
            "[280 | 682.83] loss=2.82 avg=3.05\n",
            "[290 | 706.67] loss=2.73 avg=3.04\n",
            "[300 | 730.64] loss=2.92 avg=3.03\n",
            "[310 | 754.65] loss=2.85 avg=3.02\n",
            "[320 | 778.59] loss=2.70 avg=3.01\n",
            "[330 | 802.52] loss=2.62 avg=3.00\n",
            "[340 | 826.40] loss=2.73 avg=2.99\n",
            "[350 | 850.29] loss=2.50 avg=2.97\n",
            "[360 | 874.19] loss=2.87 avg=2.97\n",
            "[370 | 898.09] loss=2.57 avg=2.96\n",
            "[380 | 921.99] loss=2.87 avg=2.95\n",
            "[390 | 945.96] loss=2.66 avg=2.94\n",
            "[400 | 970.02] loss=2.49 avg=2.93\n",
            "======== SAMPLE 1 ========\n",
            " beat them over the last 12 months!\n",
            "Thank you to the great people of Mississippi for making the election so important. Now they are going to put on a show to the world! Vote early and big numbers!\n",
            ".@BretBaier is the most overrated politician in politics in the country. I think he is not only being underestimated but hated as well!\n",
            ".@MariaBartiromo is the opposite of bad but great is @FoxNews. Don Lemon is the other one.\n",
            "Looking forward to my meeting with President Enrique Pena Nieto of Mexico in New York City on Wednesday. Great will be with Mexico!\n",
            "Thank you to all of the incredible people of New York and Florida! They will be a very special couple - and a wonderful evening.\n",
            ".@DennisKucinich is the head of NBC News and the person who will always be remembered as being the most dishonest politician in politics! I love him!\n",
            "Looking forward to meeting with Mayor Bill de Blasio of New York City in the morning at 9:30. I hope he is enjoying what he is doing!\n",
            "I watched The Apprentice last night when it was only 8:30. The cast is tremendous. You are never given credit for your work. That is important to know\n",
            "Congratulations to @GolfMagazine and all others for covering the Trump Tiers at the 2015 New England Mar-a-Lago fair. So many great people. Nice!\n",
            ".@MittRomney did really well last night – and is doing well against the strongest opponent he could have against me!\n",
            "I hope people are listening to @ericbolling and @andyrodman of @washingtonpost. We are making progress!\n",
            ".@GolfMagazine must also like and agree with the fact that \"The Apprentice\" ranked #1 in New Hampshire last night (by a lot).\n",
            "Thank you to the great men and women of @GolfMagazine for their terrific reviews and reviews on my speech at the #TennisAppetizer #TREASONEND\n",
            "Congratulations to Dan Bishop of USA in 4th place in New Hampshire. The boys are looking great this week - and I will be there!\n",
            "Will be interviewed by @seanhannity on @CNN tomorrow morning at 9:15AM @FoxNews\n",
            "Thank you Iowa! It was such a great vote!\n",
            "Just spoke with @MittRomney who said \"we have achieved many great things\" in this campaign. Very cool!\n",
            "Will be interviewed by @SarahPalinUSA tomorrow at 9:00AM. Sarah is the voice of America!\n",
            "Getting ready to leave for D.C. today for the BIG LAYER. Big day planned-big crowds planned!\n",
            "Just arrived in D.C. for the BIG LAYER. Big day planned for 4:00 ET. Will be there in two minutes!\n",
            "Going to church today for the BIG LAYER. Big crowd expected. Will be very exciting!\n",
            ".@TrumpChicago  Trump National Doral has been a top-notch venue this season. Best restaurants in nation!\n",
            "What the \"big brother\" didn't tell you is that my children- who had children with the very successful father- never give up!\n",
            "I wonder if my son @JebBush will like my campaign - the ads are very bad. Will buy  big-soros\n",
            "In addition to his brother Jebs phony $25B in personal debt and all else (in comparison to me!) Jeb Bush is in great shape- I hear you proud!\n",
            "I will be on @CNNSitTonight at 11AM ET where I discuss my family heritage &amp; my failed bid for President!\n",
            ".@WashTimes  I greatly appreciate your support - congratulations on being so successful.\n",
            "Our country is in serious economic turmoil and the U.S. tax system is being substantially reduced.\n",
            "I will be at @WashTimes  today morning for my first private endorsement. My children @BarackObama will do very well!\n",
            "Just watched the @washingtonpost get \"em.\" The media is terrible in its bias bias bias bias!\n",
            ".@KendallStewart is an outstanding woman whose very strong words in defending me will do tremendous good. I love watching her!\n",
            "I would like to thank @foxandfriends for their wonderful service. I love doing their show!\n",
            "I will be interviewed on @CNN tomorrow morning at 9:00AM @FoxNews.\n",
            "I hope everybody had a wonderful Christmas!\n",
            ".@MittRomney is doing amazing in New Hampshire. I am sure he will do even better.\n",
            "I will be on @meetthepress at 6:00 P.M. for my interview with @andrewwilling. Be there for Donald!\n",
            "It is amazing that my sons @JebBush and @megynkevin have both lost all of their money to the Clinton Cartels---I will make a major comeback!\n",
            ".@GolfMagazine  For\n",
            "\n",
            "[410 | 1004.90] loss=2.32 avg=2.91\n",
            "[420 | 1028.88] loss=2.75 avg=2.91\n",
            "[430 | 1052.89] loss=2.63 avg=2.90\n",
            "[440 | 1076.85] loss=2.55 avg=2.89\n",
            "[450 | 1100.89] loss=2.52 avg=2.88\n",
            "[460 | 1124.95] loss=2.70 avg=2.88\n",
            "[470 | 1148.96] loss=2.22 avg=2.86\n",
            "[480 | 1172.92] loss=2.34 avg=2.84\n",
            "[490 | 1196.90] loss=2.36 avg=2.83\n",
            "[500 | 1220.77] loss=2.46 avg=2.82\n",
            "Saving checkpoint/run1/model-500\n",
            "[510 | 1247.66] loss=2.27 avg=2.81\n",
            "[520 | 1271.82] loss=2.45 avg=2.80\n",
            "[530 | 1295.69] loss=2.26 avg=2.79\n",
            "[540 | 1319.64] loss=2.41 avg=2.78\n",
            "[550 | 1343.80] loss=2.59 avg=2.77\n",
            "[560 | 1367.63] loss=1.94 avg=2.75\n",
            "[570 | 1391.58] loss=1.87 avg=2.73\n",
            "[580 | 1415.64] loss=1.92 avg=2.72\n",
            "[590 | 1439.63] loss=2.27 avg=2.71\n",
            "[600 | 1463.55] loss=2.45 avg=2.70\n",
            "======== SAMPLE 1 ========\n",
            "'t been the case. In the last 3 years (2009-2012) GDP per Capita has been almost 5%. If Obama wants to be 'leaders' of our military force and economy make it into the next #MastersJt!\n",
            "In order to properly protect our troops and our Nation we must properly defend our country in all respects.  And our troops must be protected from other countries who are taking advantage of us!\n",
            "China will take care of the troops and move them to other countries where they would be better served. Over time we will stop doing this which it is doing but at some point we will have to start treating them like we treat other countries.\n",
            "So Obama who in his second term (2013-2016) has taken major steps to build up our military and military industrial complex we have a Military Threat Index of 93.6. China taking care of our troops. They are only taking advantage of our competitors and the U.S.\n",
            "In order to make proper military and military trade possible our military cannot be increased under the ridiculous deal made with China on defense of their people and companies by our leaders. They are being treated like second-class citizens!\n",
            "China has a Military Threat Index of 93.6. They have taken care of our military but they are being treated like second-class citizens. They are being treated like third-class citizens!\n",
            "In order to properly protect our troops and our Nation we must properly defend our country in all respects. And our troops must be protected from other countries who are taking advantage of us!\n",
            "China has taken care of our troops but they are being treated like second-class citizens. They are being treated like third-class citizens!\n",
            "Congratulations to @JohannaRothberg on her big #FLOTUS ratings win. @ApprenticeNBC won again. Be ready for it!\n",
            "Congratulations to Bob Kraft on his big #KegNation win. A big week for ESPN and their owner (ESPN). You have a guy who has done great!\n",
            "Looking forward to @BretBaier's #DemosInSPICshow tonight with his partner @VinceMcMahon. We love what's happening in the world. #DemosInSPIC\n",
            "Congratulations to Doug Marrone on his big news win. Doug I had a great call-up and I'm the guy who calls for him. #DemosInSPICshow\n",
            "I hope you all will watch this very important show - a great honor! @HBO\n",
            "Just watched the helicopter mess in the basement of Trump Tower. It was built years ago for very limited use. A total disaster.\n",
            "Just watched my plane crashing into the Atlantic Ocean in Miami. Not what you think when you see it in the morning.\n",
            "Why would we send 100% armed guards to a major U.S. city when our military is used to doing nothing wrong?\n",
            ".@FoxNews is really not covering me-I don’t watch them - they are hypocrites with ratings problems. They know nothing about policy. When you watch your own networks - not Fox!\n",
            "Many people thought I would run but now I never thought I would do it like this. Just like the Clintons the Republicans always win. When you watch their networks-cronyism is a problem!\n",
            "The Fake News Media made up some lies last night in order to get a “Trump” win but they do it anyway-like all others!\n",
            ".@FoxNews thinks they made up a news story about me saying I am being harassed by the NYPD while they were protecting me from terrorist.\n",
            ".@HuffingtonPost is so totally biased and has lost its way that it doesn't get reports from the real news media! @foxandfriends is all they are!\n",
            "Looking forward to discussing the importance of #DemosWithJotable tonight at 11:30 A.M. in the @TuckerCarlson Studio. Great people!\n",
            "I just got back from China where China just broke all records set while the United States was beating them at every level including currency and technology. Amazing!\n",
            "Looking forward to attending the #SWEETESTTRUMP HOUR IN A PASSAGE SERIES yet we have a winner yet to choose!\n",
            "If you are a Trump Supporter or a Supporter Who Wants To Rule Us We're sorry but there's no way you’d be eligible to win!\n",
            "Watch the new #SWEETESTTRUMP HOUR in 4 days in the studio @TuckerCarlson @FoxNews. Enjoy!\n",
            "I will be interviewed on #CNN  at  around 3 AM on @CNN. The show is great fun fun fun and exciting!\n",
            "In order to properly protect our troops and our Nation we must properly defend our country in all respects.  And our troops are being protected by China for the benefit.\n",
            "Looking forward to being on the @TuckerCarlson show this morning. So many good people will be there!\n",
            "Will be on \n",
            "\n",
            "[610 | 1498.47] loss=2.07 avg=2.69\n",
            "[620 | 1522.39] loss=2.45 avg=2.68\n",
            "[630 | 1546.29] loss=2.31 avg=2.67\n",
            "[640 | 1570.25] loss=2.30 avg=2.67\n",
            "[650 | 1594.24] loss=2.11 avg=2.65\n",
            "[660 | 1618.31] loss=2.01 avg=2.64\n",
            "[670 | 1642.30] loss=2.32 avg=2.63\n",
            "[680 | 1666.24] loss=2.25 avg=2.63\n",
            "[690 | 1690.18] loss=2.11 avg=2.62\n",
            "[700 | 1714.10] loss=1.96 avg=2.60\n",
            "[710 | 1738.01] loss=2.15 avg=2.59\n",
            "[720 | 1761.94] loss=2.28 avg=2.59\n",
            "[730 | 1785.88] loss=1.88 avg=2.57\n",
            "[740 | 1809.84] loss=2.09 avg=2.56\n",
            "[750 | 1833.98] loss=1.72 avg=2.55\n",
            "[760 | 1858.00] loss=1.90 avg=2.54\n",
            "[770 | 1881.95] loss=1.81 avg=2.52\n",
            "[780 | 1905.89] loss=1.84 avg=2.51\n",
            "[790 | 1929.85] loss=1.77 avg=2.50\n",
            "[800 | 1953.82] loss=2.05 avg=2.49\n",
            "======== SAMPLE 1 ========\n",
            ". There is only ONE Candidate who can get to the 2nd. Point is there’s not one going for Mitt. This is all about a VOTE for Bernie who has done nothing wrong. He will beat Hillary in November.\n",
            ".@megynkevinj got a few hundred more viewers than Anderson Cooper\n",
            "Anderson Cooper had a very disappointing day. He did not have the audience he should have had and probably won’t get any bigger in November! Sorry to say but @megynkevinj is a tough and smart woman with bad instincts. But she has the air!\n",
            "I will be on @donlemon tonight at 7 -7 o’clock. @FoxNews\n",
            "Wow the ratings are in for Hillary Clinton and she just had her big win in New Hampshire. We have the VICTORY for her over the years she is a force of will &amp; will do.\n",
            "I am watching highly respected former Congressman Michael McCurreat. He is going door to door. On Fox this guy does not disappoint! #Trump2016 #2016Election\n",
            "Wow the new CNN Poll has me winning New Hampshire 36% to 23%. This is mostly because of my strength of understanding and awareness around the world. People see me as the Real Estate Editor of The New York Times\n",
            "The new CNN Poll has me winning New Hampshire 37%. This is largely because of my strength of understanding and awareness around the world. People see me as the Real Estate Editor of The New York Times\n",
            "Wow the new CNN Poll has me winning New Hampshire 27%. This is largely because of my strength of understanding and awareness around the world. People see me as the Real Estate Editor of The New York Times\n",
            "Just watched Anderson Cooper on @CNN and felt like I was in his (Trump’s) first 3 episodes of All-Star @ApprenticeNBC. No longer a star. Amazing Apprentice’s 12 hour premiere last night topped all weeks this November\n",
            "Great evening with @johnhawkins on @FoxNews tonight!\n",
            "Will be discussing the economy and trade again at 9:00 P.M. @FoxNews. Many of the bad stories coming out of the White House are tragic &amp; heartbreaking!\n",
            "Wow the new CNN Poll has me winning New Hampshire 29%. This many people are seeing is mostly coming from New Hampshire. Much of it will be bad &amp; false. Hope they get more credit!\n",
            "I had breakfast with my team yesterday at Trump Tower. Made introductions and then rapidly got to work on a new set of problems!\n",
            "The U.S. trade deficit with other member states has officially reached a record high. I will announce soon whether or not we must pull out of NAFTA.\n",
            "I will be going to Scotland tomorrow for a meeting on economic growth.\n",
            "The unemployment rate for Hispanic and African American young adults has almost reached the lowest level in decades and will only get worse for years to come.\n",
            "I will be meeting with @MacMiller  in #TrumpTowers to discuss future meeting plans. Mac and I both like Trump because @Morning_Joe doesn't\n",
            "The new @CNN Poll has me winning New Hampshire 36%. So what else is new? Not much.\n",
            "What I will say when I release my new book tonight--- MAKE AMERICA GREAT AGAIN!\n",
            "The failing @nytimes should be ashamed of destroying itself in order to use their power of publicity and free advertising---that is life.\n",
            "The failing @nytimes will try and make me look as a candidate by getting ahead of my statements. But that is not very good for the people!\n",
            "This New York Post story on me --- which has nothing to do with me but lies and bad information--- is a disaster.\n",
            "So the New York Times took the lead in this dishonest and totally dishonest story on me yesterday. Now I wonder if I got ahead because it is \"like nothing else\"\n",
            "The big meeting yesterday with the @nytimes went very well. I gave lots of information on Trump and the news-stream media barely covered the substance. Very nervous! Now it is done!\n",
            "We have almost 800000 jobs in the U.S. If New York State had only treated us fairly instead of dumping us for $1.6 trillion in tariffs and other barriers! We love NY!\n",
            "It is now \"Trump Week\" in the New York Times. The great and talented Sue Gordon will be there!\n",
            "Just left Florida. Really good conversations. The worst would be Vegas. Meetings are not optimal but good things can be done.\n",
            "New York Times reporting on me and Mitt Romney. It is \"like nothing else\" --- only very badly. I was totally taken down by Mitt.\n",
            "The failing @nytimes is desperate to cover me because the failing @nytimes should be ashamed of all of the bad things they are doing!\n",
            "I would like to thank Jeffrey Lord @LordInspector for his excellent and fair review of the very bad day I celebrated!\n",
            "Will be traveling to Scotland to\n",
            "\n",
            "[810 | 1988.94] loss=1.76 avg=2.48\n",
            "[820 | 2013.03] loss=1.74 avg=2.46\n",
            "[830 | 2037.07] loss=1.68 avg=2.45\n",
            "[840 | 2061.05] loss=1.50 avg=2.43\n",
            "[850 | 2085.04] loss=1.54 avg=2.42\n",
            "[860 | 2108.97] loss=1.99 avg=2.41\n",
            "[870 | 2132.90] loss=1.48 avg=2.39\n",
            "[880 | 2156.87] loss=1.40 avg=2.38\n",
            "[890 | 2180.80] loss=1.70 avg=2.37\n",
            "[900 | 2204.75] loss=2.03 avg=2.36\n",
            "[910 | 2228.67] loss=1.77 avg=2.35\n",
            "[920 | 2252.65] loss=1.75 avg=2.34\n",
            "[930 | 2276.70] loss=1.63 avg=2.33\n",
            "[940 | 2300.72] loss=1.57 avg=2.32\n",
            "[950 | 2324.75] loss=1.46 avg=2.30\n",
            "[960 | 2348.88] loss=1.66 avg=2.29\n",
            "[970 | 2372.94] loss=1.78 avg=2.28\n",
            "[980 | 2396.83] loss=1.50 avg=2.27\n",
            "[990 | 2420.80] loss=1.41 avg=2.26\n",
            "[1000 | 2444.85] loss=1.34 avg=2.24\n",
            "Saving checkpoint/run1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `.rar` checkpoint file from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "8b1071b9-6531-475c-e2c8-6183f0b2e405"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-42fe8a91f36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_tf_sess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_gpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mload_gpt2\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, multi_gpu)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mgpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_available_gpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(hparams, X, past, scope, gpus, reuse)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n\u001b[0;32m--> 183\u001b[0;31m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.01))\n\u001b[0m\u001b[1;32m    184\u001b[0m         wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n\u001b[1;32m    185\u001b[0m                              initializer=tf.compat.v1.random_normal_initializer(stddev=0.02))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1498\u001b[0m       \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1241\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m   1244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m   def _get_partitioned_variable(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    517\u001b[0m           \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m           \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m           aggregation=aggregation)\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     synchronization, aggregation, trainable = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0;32m--> 868\u001b[0;31m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    869\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Variable model/wpe already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "7a178a73-4e33-4966-a9a1-34c9579ec7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value model/h2/mlp/c_proj/b\n\t [[{{node model/h2/mlp/c_proj/b/read}}]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5d868bf0f80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgpt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'run1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(sess, run_name, checkpoint_dir, model_name, model_dir, sample_dir, return_as_list, truncate, destination_path, sample_delim, prefix, seed, nsamples, batch_size, length, temperature, top_k, top_p, include_prefix)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnsamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             out = sess.run(output, feed_dict={\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                     \u001b[0;34m'\\nsession_config.graph_options.rewrite_options.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m                     'disable_meta_optimizer = True')\n\u001b[0;32m-> 1384\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value model/h2/mlp/c_proj/b\n\t [[node model/h2/mlp/c_proj/b/read (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n\nOriginal stack trace for 'model/h2/mlp/c_proj/b/read':\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-67d252685180>\", line 11, in <module>\n    save_every=500\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/gpt_2.py\", line 198, in finetune\n    output = model.model(hparams=hparams, X=context, gpus=gpus)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 197, in model\n    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 158, in block\n    m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 149, in mlp\n    h2 = conv1d(h, 'c_proj', nx)\n  File \"/usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/model.py\", line 84, in conv1d\n    b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0))\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1500, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 1243, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 567, in get_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 519, in _true_getter\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 933, in _get_single_variable\n    aggregation=aggregation)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 258, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 219, in _variable_v1_call\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 197, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variable_scope.py\", line 2519, in default_variable_creator\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 262, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1688, in __init__\n    shape=shape)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/variables.py\", line 1872, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/dispatch.py\", line 180, in wrapper\n    return target(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py\", line 203, in identity\n    ret = gen_array_ops.identity(input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_array_ops.py\", line 4239, in identity\n    \"Identity\", input=input, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n    attrs, op_def, compute_device)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=\"LORD\",\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "You can rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "If you want to generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}